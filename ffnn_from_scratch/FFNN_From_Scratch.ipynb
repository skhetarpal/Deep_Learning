{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Forward Neural Network\n",
    "\n",
    "Tested using the MNIST dataset\n",
    "\n",
    "Accuracy of 95% using 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features not yet included: Relu, Cost Plotting.\n",
    "class FFNN():\n",
    "    \n",
    "    def __init__(self, hidden_layers, activation_functions, cost_function, data, target):\n",
    "        \"\"\"\n",
    "        \"hidden_layers\" should be a list of hidden layer sizes in ascending order\n",
    "        \"activation_functions\" should be a list of activation functions for layer hidden_1 through output\n",
    "        \"cost_function\" should be either \"Neg Log\" or \"Sum of Squares\"\n",
    "        \"data\" should be a numpy array with m data point rows and n feature columns\n",
    "        \"target\" should be a numpy array with m data point rows and a column per class\n",
    "        \"\"\"\n",
    "        self.sizes = [np.shape(data)[1]] + hidden_layers + [np.shape(target)[1]] #List of layer sizes\n",
    "        self.activation_functions = activation_functions\n",
    "        self.cost_function = cost_function\n",
    "        self.num_classes = target.shape[1]\n",
    "        self.data = np.hstack((data, target))\n",
    "        \n",
    "        self.weights = [np.random.normal(scale = 1/np.sqrt(prev), size = (nex, prev)) \\\n",
    "                        for prev, nex in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        self.biases = [np.zeros((i,1)) for i in self.sizes[1:]]\n",
    "    \n",
    "    # Split train_data, train_target, test_data, test_target\n",
    "    def create_val_set(self, val_frac):\n",
    "        np.random.shuffle(self.data)\n",
    "        split = int(len(self.data)*(val_frac))\n",
    "        self.val_data = self.data[0:split, :]\n",
    "        self.data = self.data[split:, :]\n",
    "        \n",
    "    # Calculate the standardization metrics from the training data\n",
    "    def initial_standardization(self):\n",
    "        self.data_means = np.mean(self.data[:,:-self.num_classes], axis=0)\n",
    "        self.data_scale = np.std(self.data[:,:-self.num_classes], axis=0)\n",
    "        self.data[:,:-self.num_classes] = self.standardizeFunction(self.data[:,:-self.num_classes])\n",
    "        if self.val_frac:\n",
    "            self.val_data[:,:-self.num_classes] = self.standardizeFunction(self.val_data[:,:-self.num_classes])\n",
    "        \n",
    "    def standardizeFunction(self, data):\n",
    "        out = data\n",
    "        for column in range(data.shape[1]):\n",
    "            if self.data_scale[column]:\n",
    "                out[:,column] = (data[:,column] - self.data_means[column]) / self.data_scale[column]\n",
    "        return out\n",
    "    \n",
    "    def train(self, batch_size, alpha, epochs, val_frac = 0, standardize = True, regularization = 0):\n",
    "        self.batch_size = batch_size\n",
    "        self.val_frac = val_frac\n",
    "        self.standardize = standardize\n",
    "        self.reg = regularization\n",
    "        if self.val_frac:\n",
    "            self.create_val_set(val_frac)\n",
    "            self.val_acc = np.zeros(epochs)\n",
    "        if self.standardize:\n",
    "            self.initial_standardization()\n",
    "        self.activations = [np.zeros((layer_size, batch_size)) for layer_size in self.sizes]\n",
    "        for epoch in range(epochs):\n",
    "            print('epoch ', epoch+1)\n",
    "            np.random.shuffle(self.data) #Shuffle the data before each epoch\n",
    "            for minibatch in range(int(len(self.data)/self.batch_size)):\n",
    "                minibatch_data = self.data[minibatch*self.batch_size : (minibatch+1)*self.batch_size, :]\n",
    "                inputs = minibatch_data[:,:-self.num_classes].transpose()\n",
    "                target = minibatch_data[:,-self.num_classes:].transpose()\n",
    "                self.feedforward(inputs)\n",
    "                self.backpropagate(target)\n",
    "            if self.val_frac:\n",
    "                self.val_acc[epoch] = self.prediction_accuracy(self.val_data[:,:-self.num_classes], \\\n",
    "                                                               self.val_data[:,-self.num_classes:])\n",
    "            \n",
    "    def feedforward(self, inputs):\n",
    "        input_size = inputs.shape[1] # Not using self.batch_size so that feedforward can take test sets as well\n",
    "        self.activations[0] = inputs\n",
    "        for layer in range(len(self.weights)):\n",
    "            z = np.dot(self.weights[layer], self.activations[layer]) + np.tile(self.biases[layer], (1, input_size))\n",
    "            self.activations[layer+1] = self.activation_function(z, self.activation_functions[layer])\n",
    "    \n",
    "    def activation_function(self, z, act_fun):\n",
    "        if act_fun == \"sigmoid\":\n",
    "            return 1.0/(1.0+np.exp(-z))\n",
    "        if act_fun == \"linear\":\n",
    "            return z\n",
    "        if act_fun == \"softmax\":\n",
    "            return np.exp(z)/np.sum(np.exp(z), axis=0)\n",
    "\n",
    "    def backpropagate(self, target):\n",
    "        #Calculate dcdz for the top layer\n",
    "        dcdz = self.dcdz_top_layer(target) # Returns an n by m array\n",
    "\n",
    "        #Calculate dcdw and dcdb, and then the next dcdz\n",
    "        for layer in range(len(self.weights)-1, -1, -1): #Note that dcdz is above dcdw[layer] which is above a[layer]\n",
    "            dcdw = np.dot(dcdz, self.activations[layer].transpose())\n",
    "            self.weights[layer] = self.weights[layer] - alpha * (dcdw/self.batch_size + self.reg*self.weights[layer])\n",
    "            dcdb = np.sum(dcdz, axis=1, keepdims=True)\n",
    "            self.biases[layer] = self.biases[layer] - alpha * dcdb/self.batch_size\n",
    "            if layer > 0: #Use current layer of dcdz to calculate next layer down of dcdz\n",
    "                # Use da_lower/dz_lower * sum_over_upper(dc/dz_upper * da_lower/dz_upper)\n",
    "                dcdz = self.dadz_function(layer) * np.dot(self.weights[layer].transpose(), dcdz)\n",
    "    \n",
    "    def dadz_function(self, layer):\n",
    "        if self.activation_functions[layer-1] == \"sigmoid\":\n",
    "            return self.activations[layer] * (1.0 - self.activations[layer])\n",
    "        if self.activation_functions[layer-1] == \"linear\":\n",
    "            return self.activations[layer]\n",
    "        else: raise NameError(\"Not a valid activation function for layer \", layer)\n",
    "\n",
    "    def dcdz_top_layer(self, target):\n",
    "        if ((self.activation_functions[-1] == \"softmax\") or (self.activation_functions[-1] == \"sigmoid\")) and \\\n",
    "        (self.cost_function == \"Neg Log\"):\n",
    "            #The math works out such that the top layer derivative dcdz = activation - target\n",
    "            return self.activations[-1] - target\n",
    "        elif (self.activation_functions[-1] == \"sigmoid\") and (self.cost_function == \"Sum of Squares\"):\n",
    "            #For sigmoid with SS cost_fun, dcda = activation - target, dadc = a(1-a), dcdz = dcda*dadz\n",
    "            dcda = self.activations[-1] - target\n",
    "            return dcda * self.activations[-1] * (1 - self.activations[-1])\n",
    "        elif (self.activation_functions[-1] == \"linear\") and (self.cost_function == \"Sum of Squares\"):\n",
    "            return self.activations[-1] - target\n",
    "        else: print('Gradient for this combination of activation function and cost function is unavailable.')\n",
    "\n",
    "    #Function to predict a test set, output rows are data point, columns are one-hot vector predictions\n",
    "    def predict(self, data):\n",
    "        test_set_size = len(data)\n",
    "        if self.standardize:\n",
    "            data = self.standardizeFunction(data)\n",
    "        self.activations = [np.zeros((layer_size, test_set_size)) for layer_size in self.sizes]\n",
    "        self.feedforward(data.transpose())\n",
    "        return self.activations[-1]\n",
    "    \n",
    "    # This only works for data with one class per data point\n",
    "    def prediction_accuracy(self, data, target):\n",
    "        return np.mean(np.argmax(self.predict(data), axis = 0) == np.argmax(target, axis = 1))\n",
    "    \n",
    "def convert_to_one_hot(vector, classes = 0):\n",
    "    vector = vector.astype(np.int64)\n",
    "    rows = len(vector)\n",
    "    if not classes:\n",
    "        classes = np.unique(vector)\n",
    "    columns = len(classes)\n",
    "    out = np.zeros((rows, columns))\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            out[i,j] = classes[j] == vector[i]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'MNIST'\n",
    "\n",
    "if dataset_name == 'iris':\n",
    "    from sklearn import datasets\n",
    "    dataset = datasets.load_iris()\n",
    "elif dataset_name == 'MNIST':\n",
    "    dataset = fetch_mldata('MNIST original')\n",
    "    dataset.data = dataset.data/256\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)\n",
    "y_train = convert_to_one_hot(y_train)\n",
    "y_test = convert_to_one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1\n",
      "epoch  2\n",
      "epoch  3\n",
      "epoch  4\n",
      "epoch  5\n",
      "epoch  6\n",
      "epoch  7\n",
      "epoch  8\n",
      "epoch  9\n",
      "epoch  10\n",
      "accuracy is  0.9545714285714286\n",
      "Training time was  20.510340929031372\n",
      "Validation set accuracy over epochs was  [0.90839286 0.92517857 0.933125   0.93928571 0.94455357 0.94651786\n",
      " 0.94866071 0.95053571 0.95142857 0.95303571]\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [30]\n",
    "activation_functions = [\"sigmoid\", \"softmax\"]\n",
    "cost_function = \"Neg Log\"\n",
    "batch_size = 20\n",
    "alpha = 0.1\n",
    "epochs = 10\n",
    "val_frac = 0.2\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "network = FFNN(hidden_layers = hidden_layers, activation_functions = activation_functions, \\\n",
    "               cost_function = cost_function, data = x_train, target = y_train)\n",
    "network.train(batch_size = batch_size, alpha = alpha, epochs = epochs, val_frac = val_frac, \\\n",
    "              standardize = False, regularization = 0)\n",
    "print(\"accuracy is \", network.prediction_accuracy(data = x_test, target = y_test))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Training time was \", end-start)\n",
    "if val_frac:\n",
    "    print(\"Validation set accuracy over epochs was \", network.val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
